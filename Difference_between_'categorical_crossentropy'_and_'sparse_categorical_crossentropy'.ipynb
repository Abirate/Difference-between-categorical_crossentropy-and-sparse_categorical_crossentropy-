{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Difference between 'categorical_crossentropy' and 'sparse_categorical_crossentropy'.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fwmKbDylxw"
      },
      "source": [
        "\r\n",
        "#  **The difference between :**\r\n",
        "**'sparse_categorical_crossentropy'** and\r\n",
        "**'categorical_crossentropy'**\r\n",
        "(With an example)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVgYsCvxr_UL"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#  **Before Starting:**\r\n",
        "\r\n",
        "To Load The Data (that I use in this demonstration) :\r\n",
        "\r\n",
        "See the first cells of the Notebook: **week 4** of the course 2 : **Convolutional Neural Networks in TensorFlow**\r\n",
        "\r\n",
        "of **DeepLearning.AI TensorFlow Developer Professional Certificate**. here the link ot the course: https://www.coursera.org/learn/convolutional-neural-networks-tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgC6eXFw7i3q"
      },
      "source": [
        "\r\n",
        "> **Description**\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRZzfqUAz9tH"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> From **First Step** to **Third Step**:\r\n",
        "\r\n",
        "\r\n",
        "I do a series of treatments on image files,(+checking the convolutional model: **compiling it with different** parameters values to show the difference between the two values of **loss parameter** : \r\n",
        "\r\n",
        "**'sparse_categorical_crossentropy'**  and **'categorical_crossentropy'** \r\n",
        "\r\n",
        "This demonstration **doesn't use** the **image generators**(you will understand why if you follow the 3 different steps)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> ***Just in passing:*** \r\n",
        "\r\n",
        "If you try to train the model of the course with **train_generator** and **validation_generator**(but not with the value of the course, but by setting the value of **loss = 'sparse_categorical_crossentropy'**,(instead of **'categorical_crossentropy'**: \r\n",
        "\r\n",
        "this gives an error (you can try it):\r\n",
        "\r\n",
        "**The message of the error:**\r\n",
        "\r\n",
        "**'InvalidArgumentError:** logits and labels must have the same first dimension, got logits shape [126,3] and labels shape [378]' \r\n",
        "\r\n",
        "So, it only works with **loss ='categorical_crossentropy '**): because in our case: it is **the generator** that **labels** all the images fotr us(...)\r\n",
        "\r\n",
        "\r\n",
        "> Does this mean that **'sparse_categorical_crossentropy'** never works with **generators**?\r\n",
        "\r\n",
        "\r\n",
        "**Loud No:** the image generaors can work with **sparse_categorical_crossentropy'**, only in this case:\r\n",
        "\r\n",
        "When  **y_train**, or **y_test** (the vectors of our labels) are not included in **train_generator**, and **validation_generator**) successively, that's means in the case where there is **separation** between : **X_train & y_train** (of the train dataset ), and **X_test & y_test** (of the test dataset): so this doesn't prevent the application of  **generators** (on them...) and we **must** in this case use **'sparse_categorical_crossentropy'**(if -of course-  y_train and y_test haven't already been reshaped (by us)...(**see the demonstration, below  to figure out it with a pracical example)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHHcZqH-mSY"
      },
      "source": [
        "\r\n",
        "#  **First_Step**:  \r\n",
        "we create (**X_train, y_train)**, **(X_test,y_test)** from the image files **without using generators** in order to transform them in numpy arrays\r\n",
        "\r\n",
        "**Note:** generatos do this work for us , but we need **X_train** (features) and **y_train** (labels) separated (for the rest...)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5nh_5Lbwbq"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> **I-1 Creating (X_train, y_train)**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxmuCTs9xI4g"
      },
      "source": [
        "#Note: \r\n",
        "#I already did zip_ref.extractall('/content') and not on /tmp, that's why you will see /content instead of /tmp \r\n",
        "\r\n",
        "rock_dir = os.path.join('/content/rps/rock')\r\n",
        "paper_dir = os.path.join('/content/rps/paper')\r\n",
        "scissors_dir = os.path.join('/content/rps/scissors')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kDe6f5FuTmd"
      },
      "source": [
        "from matplotlib.image import imread\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "paper_dir = os.path.join('/content/rps/paper')\r\n",
        "rock_dir = os.path.join('/content/rps/rock')\r\n",
        "scissors_dir = os.path.join('/content/rps/scissors')\r\n",
        "\r\n",
        "\r\n",
        "list_path_images_train_paper= [os.path.join(paper_dir,fn) for fn in os.listdir(paper_dir)]\r\n",
        "list_path_images_train_rock= [os.path.join(rock_dir,fn) for fn in os.listdir(rock_dir)]\r\n",
        "list_path_images_train_scissors = [os.path.join(scissors_dir,fn) for fn in os.listdir(scissors_dir)]\r\n",
        "\r\n",
        "#We transform the images in numpy arrays\r\n",
        "train_paper = [imread(path_image) for path_image in list_path_images_train_paper]\r\n",
        "train_paper= np.array(train_paper)\r\n",
        "\r\n",
        "# in order to get the shape (840, 300, 300, 3)\r\n",
        "train_paper= train_paper[:,:,:,:3]\r\n",
        "\r\n",
        "#We define the labels\r\n",
        "#Note: with the generator version if we do : \r\n",
        "#train_generator.class_indices, it gives us : {'paper': 0, 'rock': 1, 'scissors': 2} (So we keep this order)\r\n",
        "y_train_paper = np.zeros(840)\r\n",
        "\r\n",
        "#we repeat the same treatment with rock subdirectory\r\n",
        "train_rock = [imread(path_image) for path_image in list_path_images_train_rock]\r\n",
        "train_rock= np.array(train_rock)\r\n",
        "# in order to get the shape (840, 300, 300, 3)\r\n",
        "train_rock= train_rock[:,:,:,:3]\r\n",
        "#we define the labels\r\n",
        "y_train_rock = np.ones(840)\r\n",
        "\r\n",
        "#we repeat the same treatment with scissors subdirectory\r\n",
        "train_scissors = [imread(path_image) for path_image in list_path_images_train_scissors]\r\n",
        "train_scissors= np.array(train_scissors)\r\n",
        "#in order to get the shape (840, 300, 300, 3)\r\n",
        "train_scissors= train_scissors[:,:,:,:3]\r\n",
        "#we define the labels\r\n",
        "y_train_scissors = (np.ones(840))*2\r\n",
        "\r\n",
        "X_train= np.concatenate((train_paper,train_rock,train_scissors))\r\n",
        "y_train= np.concatenate((y_train_paper,y_train_rock,y_train_scissors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYsQjOtb2jKB",
        "outputId": "3fb8e733-9b1e-4fc2-ac6b-6e476822bc7e"
      },
      "source": [
        "#just a little check\r\n",
        "X_train.shape\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2520, 300, 300, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0bJYWxG4nP5"
      },
      "source": [
        "#We have two separated numpy arrays X_train and y_train with the same length (but different shapes).\r\n",
        "#The elements are not shuffled (we have the elments of class 0 (paper))at the beginning, then come the elements\r\n",
        "#of class 1 (rock), then the elements of class 2 (scissors)\r\n",
        "#To solve this, we want to shuffle each of them, such that corresponding elements continue to correspond.\r\n",
        "#we define this function:\r\n",
        "\r\n",
        "def shuffle_same_order(X, y):\r\n",
        "    assert len(X) == len(y)\r\n",
        "    shuffled_X = np.empty(X.shape, dtype=X.dtype)\r\n",
        "    shuffled_y = np.empty(y.shape, dtype=y.dtype)\r\n",
        "    permutation = np.random.permutation(len(X))\r\n",
        "    for old_index, new_index in enumerate(permutation):\r\n",
        "        shuffled_X[new_index] = X[old_index]\r\n",
        "        shuffled_y[new_index] = y[old_index]\r\n",
        "    return shuffled_X, shuffled_y\r\n",
        "\r\n",
        "#Note: assert statement has a condition or expression which is supposed to be always true. \r\n",
        "#If the condition is false assert halts the program and gives an AssertionError."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws36WlHA6JWX"
      },
      "source": [
        "(X_train,y_train) = shuffle_same_order(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBRJ7PFNcElK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> **I-2 Creating (X_test, y_test)**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRRL1bK36kxg"
      },
      "source": [
        "#To build (X_test and y_test), we do the same treatment for test_datset (validation dateset)\r\n",
        "\r\n",
        "paper_dir_test = os.path.join('/content/rps-test-set/paper')\r\n",
        "rock_dir_test = os.path.join('/content/rps-test-set/rock')\r\n",
        "scissors_dir_test = os.path.join('/content/rps-test-set/scissors')\r\n",
        "\r\n",
        "list_path_images_test_paper= [os.path.join(paper_dir_test,fn) for fn in os.listdir(paper_dir_test)]\r\n",
        "list_path_images_test_rock= [os.path.join(rock_dir_test,fn) for fn in os.listdir(rock_dir_test)]\r\n",
        "list_path_images_test_scissors = [os.path.join(scissors_dir_test,fn) for fn in os.listdir(scissors_dir_test)]\r\n",
        "\r\n",
        "#We transform the images in numpy arrays\r\n",
        "test_paper = [imread(path_image) for path_image in list_path_images_test_paper]\r\n",
        "test_paper= np.array(test_paper)\r\n",
        "\r\n",
        "#in order to get the shape (124, 300, 300, 3)\r\n",
        "test_paper= test_paper[:,:,:,:3]\r\n",
        "#we define the labels\r\n",
        "y_test_paper = np.zeros(124)\r\n",
        "\r\n",
        "#we repeat the same treatment with rock subdirectory\r\n",
        "test_rock = [imread(path_image) for path_image in list_path_images_test_rock]\r\n",
        "test_rock= np.array(test_rock)\r\n",
        "# to get the shape (124, 300, 300, 3)\r\n",
        "test_rock= test_rock[:,:,:,:3]\r\n",
        "#we define the labels\r\n",
        "y_test_rock = np.ones(124)\r\n",
        "\r\n",
        "#we repeat the same treatment with scissors subdirectory\r\n",
        "test_scissors = [imread(path_image) for path_image in list_path_images_test_scissors]\r\n",
        "test_scissors= np.array(test_scissors)\r\n",
        "# to get the shape (124, 300, 300, 3)\r\n",
        "test_scissors= test_scissors[:,:,:,:3]\r\n",
        "#we define the labels\r\n",
        "y_test_scissors = (np.ones(124))*2\r\n",
        "\r\n",
        "X_test= np.concatenate((test_paper,test_rock,test_scissors))\r\n",
        "y_test= np.concatenate((y_test_paper,y_test_rock,y_test_scissors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zux9yCMX7gYO"
      },
      "source": [
        "(X_test,y_test) = shuffle_same_order(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRCU_lFcSDw"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#  **Second Step:  Creating the model**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THOSIszm-f3C"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "#create our model (the same that we created for training the train_generator), but we reduce the filters\r\n",
        "#for convolutions because, we have the original shape (300,300,3), not (150,150,3)\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "    \r\n",
        "    # This is the first convolution\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\r\n",
        "    # The second convolution\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The third convolution\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # The fourth convolution\r\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    # Flatten the results to feed into a DNN\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dropout(0.5),\r\n",
        "    # 512 neuron hidden layer\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\r\n",
        "])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afAmM4PrM-nn"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> I- We will compile the model with     **'categorical_crossentropy'**, without any modification in labels vectors shape: y_train and y_test\r\n",
        "\r\n",
        "we will end up with an **error** when we will **train** the model ! (**see the error when training the model in Third step)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EITgV5ab_O7O"
      },
      "source": [
        "#we will compile our model with loss ='categorical_crossentropy'\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4M1pkjAcsdP"
      },
      "source": [
        "\r\n",
        "\r\n",
        "#  **Third Step: Problem and solutions:**\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LEvdZu-QdWz"
      },
      "source": [
        "#below, when we train the model, we end up with an error \r\n",
        "#because the shape of y_train and y_test are vectors like this:\r\n",
        "# y_train = array([0., 2., 0., ..., 1., 0., 2.])\r\n",
        "# but the shape should become like this:\r\n",
        "# array([[1., 0., 0.],\r\n",
        "#        [0., 1., 0.],\r\n",
        "#        [1., 0., 0.],\r\n",
        "#         ...,\r\n",
        "#        [0., 0., 1.],\r\n",
        "#        [0., 0., 1.],\r\n",
        "#        [0., 1., 0.]], dtype=float32)\r\n",
        "#in order to distinguish classes (categorical values) from continuous values \r\n",
        "\r\n",
        "# To solve this problem, we have two solutions :\r\n",
        "\r\n",
        "# ---Solution_1:  we reshape y_train and y_test as mentioned above.\r\n",
        "# ---Solution_2:  we remplace 'categorical_crossentropy' with'sparse_categorical_crossentropy\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "BZmC59STOrCr",
        "outputId": "4bcc3500-2a18-4c2c-d814-c9fb9eaa555c"
      },
      "source": [
        "#The error we get:\r\n",
        "model.fit(X_train, y_train, epochs=25, validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1d478cf80981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz6YEvF0TZv7"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "> **III-1 Solution_1**: \r\n",
        "\r\n",
        "we **reshape** **y_train** and **y_test** as mentioned above & but we keep the same parameter: **'categorical_crossentropy'**\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHjcELGYB4vI"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\r\n",
        "y_train = to_categorical(y_train)\r\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFlo3QqtAfAI",
        "outputId": "8774f173-56b9-4e19-d6c2-58758775e1ad"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrjdL4MCDFr9",
        "outputId": "8fae22f2-c359-44f8-edc3-d1413e3b4ada"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTXJJgUIVt-M"
      },
      "source": [
        "#Note, we shouldn't use the model that we used above (I mean the one we  already trained above \r\n",
        "#and gives us an  error) # : we should run and re-compile the model a 2nd time then we train it  as following:\r\n",
        "\r\n",
        "#Note: \r\n",
        "#we are not trying here to see if the accuracy is good or not\r\n",
        "#The goal, here is just to figure out when we can use \r\n",
        "#'sparse_categorical_crossentropy' or 'categorical_crossentropy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMzoUiyyVJm9",
        "outputId": "7ff2f4db-9117-4a31-f6e8-f7b30f032e29"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 219s 3s/step - loss: 2.5304 - accuracy: 0.5879 - val_loss: 1.7807 - val_accuracy: 0.7231\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 218s 3s/step - loss: 0.1382 - accuracy: 0.9833 - val_loss: 2.1031 - val_accuracy: 0.7634\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 218s 3s/step - loss: 0.2231 - accuracy: 0.9965 - val_loss: 1.2695 - val_accuracy: 0.7957\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.8601 - val_accuracy: 0.7796\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 1.6458 - accuracy: 0.9556 - val_loss: 1.5597 - val_accuracy: 0.8038\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 0.2821 - accuracy: 0.9931 - val_loss: 1.6363 - val_accuracy: 0.7661\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 6.1551e-05 - accuracy: 1.0000 - val_loss: 2.2484 - val_accuracy: 0.7688\n",
            "Epoch 8/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 0.4176 - accuracy: 0.9806 - val_loss: 2.5674 - val_accuracy: 0.7930\n",
            "Epoch 9/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 1.2625e-05 - accuracy: 1.0000 - val_loss: 2.8505 - val_accuracy: 0.7823\n",
            "Epoch 10/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 0.0979 - accuracy: 0.9959 - val_loss: 2.5855 - val_accuracy: 0.7527\n",
            "Epoch 11/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 7.1660e-05 - accuracy: 1.0000 - val_loss: 2.8644 - val_accuracy: 0.7715\n",
            "Epoch 12/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 3.1530e-06 - accuracy: 1.0000 - val_loss: 3.3048 - val_accuracy: 0.7581\n",
            "Epoch 13/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 1.3108e-07 - accuracy: 1.0000 - val_loss: 4.0248 - val_accuracy: 0.7688\n",
            "Epoch 14/20\n",
            "79/79 [==============================] - 216s 3s/step - loss: 1.2265e-09 - accuracy: 1.0000 - val_loss: 4.4962 - val_accuracy: 0.7634\n",
            "Epoch 15/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 0.0417 - accuracy: 0.9981 - val_loss: 5.4603 - val_accuracy: 0.7097\n",
            "Epoch 16/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 5.9453e-06 - accuracy: 1.0000 - val_loss: 5.2180 - val_accuracy: 0.7204\n",
            "Epoch 17/20\n",
            "79/79 [==============================] - 218s 3s/step - loss: 1.0629e-07 - accuracy: 1.0000 - val_loss: 5.3241 - val_accuracy: 0.7177\n",
            "Epoch 18/20\n",
            "79/79 [==============================] - 218s 3s/step - loss: 0.3403 - accuracy: 0.9922 - val_loss: 5.1553 - val_accuracy: 0.7715\n",
            "Epoch 19/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 1.2872e-07 - accuracy: 1.0000 - val_loss: 5.1666 - val_accuracy: 0.7688\n",
            "Epoch 20/20\n",
            "79/79 [==============================] - 217s 3s/step - loss: 1.8236e-09 - accuracy: 1.0000 - val_loss: 5.1955 - val_accuracy: 0.7688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f508b0251d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N49QmV4URvAN"
      },
      "source": [
        "\r\n",
        "\r\n",
        "> **III-2 Solution 2**:\r\n",
        "\r\n",
        " we just replace **'categorical_crossentropy'** by **'sparse_categorical_crossentropy'** (and **keeping the original shape of y_train and y_test**)\r\n",
        "\r\n",
        "==> So the difference between **solution 1** and **solution 2**: that in this solution 2 :  **'sparse_categorical_crossentropy'** does the work that we already did in **reshaping** : y_train & y_test, in solution 1 (here it does automatically, the work for us) \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs0epWnOfSGm"
      },
      "source": [
        "# First : we (re)run the code cell of our model (again)\r\n",
        "#and we use the code (below) for compiling it (So with 'Sparse_categorical_crossentropy' and not 'categorical_crossentropy')\r\n",
        "#for this we sohould use the original forms of y_train and y_test (before reshaping them)\r\n",
        "#I mean this original shape array([0., 2., 0., ..., 1., 0., 2.])\r\n",
        "#So I just run the code cells (of step1 ), that gives us the origial y_train and y_test \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkqRTsUgn2eF",
        "outputId": "781ecab8-75f4-4340-a73d-d5301cee1446"
      },
      "source": [
        "# And now, Just for checking (after running the code cells of step 1):\r\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., ..., 1., 2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCQgemeofWf2"
      },
      "source": [
        "#The most important thing here: thar we  compile our model with loss ='sparse_categorical_crossentropy'\r\n",
        "#and we will see: we won't end up with an error while training the model\r\n",
        "\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer= 'rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYEDHKWWoWX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b112be9-dc3e-45ea-d221-f5d6a2e9248d"
      },
      "source": [
        "#Note, I set epochs =5 (to make the simulation faster)\r\n",
        "#Because the goal here is'nt to check the performance of the model \r\n",
        "#But the use of 'categorical_crossentropy', and 'sparse_categorical_crossentropy'\r\n",
        "\r\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "79/79 [==============================] - 218s 3s/step - loss: 2.0025 - accuracy: 0.5474 - val_loss: 0.7967 - val_accuracy: 0.6129\n",
            "Epoch 2/5\n",
            "79/79 [==============================] - 218s 3s/step - loss: 0.0998 - accuracy: 0.9711 - val_loss: 0.4762 - val_accuracy: 0.8522\n",
            "Epoch 3/5\n",
            "79/79 [==============================] - 216s 3s/step - loss: 0.0982 - accuracy: 0.9842 - val_loss: 0.8167 - val_accuracy: 0.7849\n",
            "Epoch 4/5\n",
            "79/79 [==============================] - 218s 3s/step - loss: 0.0290 - accuracy: 0.9964 - val_loss: 0.5804 - val_accuracy: 0.7796\n",
            "Epoch 5/5\n",
            "79/79 [==============================] - 217s 3s/step - loss: 7.5096e-04 - accuracy: 1.0000 - val_loss: 1.0288 - val_accuracy: 0.8011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f004e92d358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}